# 实验日志（kg-RAG 项目）

## 2025-11-12 实验：小玩具数据跑通流程
- 目的：先把整条流程跑通，看代码能不能工作
- 数据：
  - docs.jsonl 里只有 2 个小段落（Beatrice I + Penicillin）
- 操作：
  - `python src/build_index.py`
  - `python src/answer.py "What was the nationality of Beatrice I's husband?" 5`
- 结果：
  - 系统返回答案：German
  - Top-5 里有正确段落
- 结论/备注：
  - 管道整体没问题：加载模型 -> 抽实体建图 -> 检索 -> 回答 都跑通了。

---

## 2025-11-12 实验：抓 PubMed 做医学语料（失败/部分成功）
- 目的：用 PubMed 做一个真实一点的医疗语料库
- 数据：
  - 调用 `src/prepare_pubmed.py` 去 PubMed 拉摘要
- 操作：
  - 一开始 `python src/prepare_pubmed.py 500`，遇到大量 HTTP 400 错误
  - 后来改成多通道兜底逻辑 + 200 条，能写出 `data/docs.jsonl`，但里面关于 “Staphylococcus / MRSA” 的内容很少
- 结果：
  - 虽然程序跑完了，但内容不符合我要的「**重点是金黄色葡萄球菌 + 抗生素**」这个目标
- 结论/备注：
  - 这版语料**不适合作为最终实验语料**，只当是一次抓数据的尝试，留个记录。

---

## 2025-11-13 实验：手工筛选 299 篇 Staph+抗生素语料
- 目的：做一个小而精的医学子语料，用来对比 BM25 和 KG+PPR
- 数据：
  - `data/docs.jsonl`
  - 共 299 条
  - 检查结果：
    - 含 Staph 相关关键词的条数：297
    - 含常见抗生素名字的条数：297
- 操作：
  1. 用脚本统计语料情况（字符数、关键词等）
  2. `python src/build_index.py` 重新建索引
     - 使用 `en_core_sci_md-0.5.4`（SciSpacy 医学模型）
     - spaCy 版本：3.7.5
  3. 用以下几种检索/回答方式：
     - 纯 BM25：`answer_bm25.py`
     - KG+PPR 检索：`ppr_retrieve.py`
     - KG+PPR + 关键词过滤：`ppr_kw_retrieve.py`
     - 药物列表抽取：`answer_drugs.py`
- 结果（单个问题：  
  “Which antibiotics are commonly used to treat *Staphylococcus aureus* (including MRSA) infections?”）：
  - 你定义的 GOLD（标准答案）有 17 个药名
  - `python src/eval_compare.py` 输出：
    - BM25：P=1.0, R=0.882, F1=0.938（少找了 2 个）
    - KG+PPR：P=1.0, R=1.0, F1=1.0（17 个全找到了）
  - `python src/eval_grid.py` 输出（不同 Top-K）：
    - 不管 K=5/10/15/20/30/50：
      - BM25 的 Recall 最高到 0.882
      - KG+PPR 最好可以做到 Recall=1.0，F1=1.0
- 结论/备注：
  - 在 **同一套语料 + 同一个问题 + 同一套 GOLD 列表** 下，
    KG+PPR 检索 + 药名抽取明显比纯 BM25 更全面（能多找出 dicloxacillin、flucloxacillin 等）。
  - 这是你后面写论文时可以重点展示的一条「对比实验证据」。

## 实验 3：向量检索（Vec+Drugs）在抗菌药物问答上的效果

**时间：**（填你今天的日期）  
**环境：** Windows + Conda 环境 `kg-rag`  
**代码：** `kg_rag_demo` 仓库

### 3.1 语料与索引

- 语料：`data/docs.jsonl`（PubMed 抽取的 300 篇与感染/抗生素相关摘要）
- 向量模型：`sentence-transformers/all-MiniLM-L6-v2`
- 向量索引：
  - 通过 `python src/build_vec_index_vec.py` 生成
  - 向量文件：`data/index_vec_emb.npy`
  - 元信息：`data/index_vec_meta.json`

### 3.2 评测设置

- 问题文件：`data/qa_med_questions.jsonl`（共 14 个问题：7 个英文 + 7 个中文）
- 标准答案：每个问题给定一个“标准抗生素列表”（gold_drugs）
- 系统：向量检索 + 药物名称抽取（Vec+Drugs）
  - 检索：对问题做向量编码，与 300 篇文献做相似度检索
  - 取前 `Top-K=20` 篇文献，拼接摘要
  - 从拼接文本中用规则匹配药物名称，得到预测集合（pred_drugs）
- 评价指标：
  - 对每个问题：
    - TP / FP / FN
    - Precision, Recall, F1
  - 汇总：
    - 微平均（micro）
    - 宏平均（macro）
- 运行命令：
  ```bash
  python src/qa_med_eval_vec.py

---

## 4. 接下来可以干啥？

你现在已经有了：

- KG+PPR 的全套评测结果（`qa_med_eval.csv`）  
- 向量检索 Vec+Drugs 的全套评测结果（`qa_med_eval_vec.csv`）  
- 单问题（MRSA 那题）的 BM25 vs KG+PPR 的详细对比和 F1 曲线（`eval_grid.py` 那批）

下一步比较自然的几条路：

1. **做一个总对比表：同一批问题，三种方法并排：BM25 / KG+PPR / Vec**  
   - 以后写论文时，一张表就能把“传统检索 vs KG 检索 vs 向量检索”的差异讲清楚。

2. **再试一个“医学专用向量模型”**（比如 BioClinicalBERT 那类），看看能不能把向量版的 Precision 拉高一点。  
   ——当然，这一步可能会对网速 / GPU 有点要求，可以慢慢来。

3. **开始整理“实验设计”那一章的框架：**
   - 实验 1：合成小语料 + QA（你一开始的 Beatrice / Penicillin 那个）  
   - 实验 2：PubMed + KG+PPR 药物问答  
   - 实验 3：PubMed + 向量检索药物问答  
   - 后面可以加：融合方法（KG+Vec）作为“改进版”。

你现在这个进度，用一句直白的话总结就是：

> 你已经不只是“把别人代码跑通”，  
> 而是**在实打实地比较三种检索路径在医疗 QA 场景下的优劣**，  
> 这是可以写进论文正文的实打实内容。
### 表 X：抗菌药物推荐任务上不同检索方法的整体表现（qa_med_questions.jsonl，14 个问题）

| 方法        | Micro Precision | Micro Recall | Micro F1 | Macro Precision | Macro Recall | Macro F1 |
|------------|-----------------|--------------|----------|-----------------|--------------|----------|
| KG+PPR     | 0.215           | 0.289        | 0.246   | 0.225           | 0.243        | 0.202   |
| 向量检索   | 0.223           | 0.322        | 0.264   | 0.221           | 0.322        | 0.236   |

### 表 X  不同检索方法在抗菌药物推荐任务上的整体表现

| 方法          | 检索思路            | P_micro | R_micro | F1_micro | P_macro | R_macro | F1_macro |
|---------------|---------------------|---------|---------|----------|---------|---------|----------|
| KG+PPR        | 知识图谱 + 随机游走 | 0.215   | 0.289   | 0.246    | 0.225   | 0.243   | 0.202    |
| 向量检索 Vec  | 语义向量检索       | 0.223   | 0.322   | 0.264    | 0.221   | 0.322   | 0.236    |
| BM25          | 关键词匹配检索     | 0.284   | 0.233   | 0.256    | 0.251   | 0.237   | 0.187    |

在构建了以细菌感染及抗菌药物为核心的领域问答集后，我们分别采用三类检索策略进行抗菌药物列表预测任务的评测：基于知识图谱的 KG+PPR 检索、基于句向量的语义检索（Vec）以及传统 BM25 关键词检索。整体结果如表 X 所示。

从微平均指标（micro-average）来看，三种方法的 F1 分数均在 0.24–0.26 左右，处于同一量级。其中，BM25 的精确率（P_micro=0.284）最高，说明其输出的药物中“命中标准答案”的比例相对更高，但召回率较低（R_micro=0.233），存在一定漏检；向量检索的召回率最高（R_micro=0.322），能够覆盖更多标准药物，但精确率略低（P_micro=0.223），预测结果中包含更多“多余药物”；KG+PPR 的表现介于两者之间（P_micro=0.215，R_micro=0.289），在当前数据规模下尚未体现出显著优势。

宏平均指标（macro-average）进一步反映了不同问题之间的稳定性。向量检索在宏平均召回（R_macro=0.322）上同样优于 BM25 和 KG+PPR，提示其在不同类型问题上的覆盖能力更强；而 BM25 的宏平均精确率（P_macro=0.251）相对较高，KG+PPR 在宏平均 F1（0.202）上略低于另外两种方法。总体而言，三种方法均能够在一定程度上从文献中恢复出合理的抗菌药物组合，但仍存在较大的改进空间，为后续设计结合知识图谱结构与语义信息的改进模型提供了基线参考。

| 方法 | 微平均 P | 微平均 R | 微平均 F1 | 宏平均 P | 宏平均 R | 宏平均 F1 |
|---|---|---|---|---|---|---|
| BM25 | 28.4% | 23.3% | 25.6% | 25.1% | 23.7% | 18.7% |
| Vec | 22.3% | 32.2% | 26.4% | 22.1% | 32.2% | 23.6% |
| KG+PPR | 21.5% | 28.9% | 24.6% | 22.5% | 24.3% | 20.2% |
| UnionAll | 20.9% | 42.2% | 27.9% | 21.0% | 39.7% | 24.9% |
| Majority2 | 25.7% | 30.0% | 27.7% | 27.8% | 29.2% | 24.1% |
