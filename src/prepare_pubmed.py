# src/prepare_pubmed.py â€”â€” ä¸‰é‡å…œåº•æŠ“å–ç‰ˆ
# åŠŸèƒ½ï¼šä» PubMed æ‰¹é‡æŠ“â€œé¢˜ç›®+æ‘˜è¦â€ï¼Œå†™å…¥ data/docs.jsonlï¼ˆè¦†ç›–æ—§æ–‡ä»¶ï¼‰

import os, sys, time, json, html
from typing import List, Iterable
from urllib.parse import urlencode
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import xml.etree.ElementTree as ET

from Bio import Entrez

# å¿…å¡«ï¼šçœŸå®é‚®ç®±ï¼ˆNCBI è¦æ±‚ï¼‰
Entrez.email = "windwait0@gmail.com"
# å¯å¡«ä½ çš„ NCBI API Keyï¼ˆæ²¡æœ‰å°±ç•™ç©ºï¼‰
Entrez.api_key = os.environ.get("NCBI_API_KEY", "")

# ä¸»é¢˜å…³é”®è¯ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
QUERY = '(pneumonia[Title/Abstract]) OR ("Streptococcus pneumoniae"[Title/Abstract]) OR (pneumococcal[Title/Abstract]) OR (antibiotic[Title/Abstract])'

UA_HDR = {"User-Agent": f"kg-rag-demo/1.0 ({Entrez.email})"}

# ============== æœç´¢ï¼ˆå…ˆ XMLï¼Œå¤±è´¥è½¬ JSONï¼‰ =================
def search_pmids(query: str, retmax: int) -> List[str]:
    # 1) Entrez XML
    for attempt in range(3):
        try:
            h = Entrez.esearch(db="pubmed", term=query, retmax=retmax, sort="relevance", retmode="xml")
            r = Entrez.read(h)
            ids = r.get("IdList", [])
            if ids:
                return ids
        except Exception as e:
            print(f"âš ï¸ esearch XML å°è¯• {attempt+1}/3 å¤±è´¥ï¼š{e}")
            time.sleep(0.7 * (attempt + 1))
    # 2) å¤‡ç”¨ JSON
    try:
        params = dict(db="pubmed", term=query, retmax=str(retmax), sort="relevance", retmode="json")
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?" + urlencode(params)
        data = urlopen(Request(url, headers=UA_HDR), timeout=30).read()
        j = json.loads(data)
        return j.get("esearchresult", {}).get("idlist", [])
    except Exception as e:
        print(f"âŒ esearch JSON ä¹Ÿå¤±è´¥ï¼š{e}")
        return []

# ============== è§£æ XML çš„å°å·¥å…· =================
def _parse_pubmed_xml(xbytes: bytes) -> Iterable[dict]:
    """è§£æ PubMed efetch çš„ XMLï¼šæå– PMIDã€Titleã€AbstractText"""
    root = ET.fromstring(xbytes)
    # PubmedArticleSet / PubmedArticle
    for art in root.findall(".//PubmedArticle"):
        pmid_el = art.find(".//MedlineCitation/PMID")
        pmid = pmid_el.text.strip() if pmid_el is not None and pmid_el.text else None
        art_node = art.find(".//MedlineCitation/Article")
        if art_node is None or not pmid:
            continue
        # æ ‡é¢˜
        title_el = art_node.find("./ArticleTitle")
        title = ""
        if title_el is not None:
            title = "".join(title_el.itertext()).strip()
        # æ‘˜è¦ï¼ˆå¯èƒ½å¤šæ®µï¼‰
        abstract_texts = []
        for at in art_node.findall("./Abstract/AbstractText"):
            abstract_texts.append("".join(at.itertext()).strip())
        abstract = " ".join([t for t in abstract_texts if t])
        text = " ".join([title, abstract]).strip()
        if not text:
            continue
        text = html.unescape(text).replace("\n", " ").strip()
        yield {"id": f"pmid_{pmid}", "text": text}

# ============== æŠ“å–ä¸€æ‰¹ï¼šä¸‰é‡å…œåº• =================
def _efetch_entrez_xml(id_list: List[str]) -> List[dict]:
    """ä¼˜å…ˆï¼šBiopython Entrez XML"""
    try:
        h = Entrez.efetch(db="pubmed", id=",".join(id_list), rettype="abstract", retmode="xml")
        r = Entrez.read(h)  # å…ˆç”¨ biopython è§£æï¼Œè‹¥å¤±è´¥å†èµ°æˆ‘ä»¬è‡ªå·±çš„è§£æ
        out = []
        for art in r.get("PubmedArticle", []):
            pmid = str(art["MedlineCitation"]["PMID"])
            art_info = art["MedlineCitation"]["Article"]
            title = " ".join(art_info.get("ArticleTitle", "")) if isinstance(art_info.get("ArticleTitle", ""), list) else str(art_info.get("ArticleTitle", ""))
            abs_parts = art_info.get("Abstract", {}).get("AbstractText", [])
            abstract = " ".join([str(x) for x in abs_parts]) if isinstance(abs_parts, list) else str(abs_parts)
            text = " ".join([title, abstract]).strip()
            if text:
                text = html.unescape(text).replace("\n", " ").strip()
                out.append({"id": f"pmid_{pmid}", "text": text})
        return out
    except Exception as e:
        print(f"  â†ªï¸ Entrez XML å¤±è´¥ï¼š{e}")
        return []

def _efetch_http_xml(id_list: List[str]) -> List[dict]:
    """å¤‡ç”¨1ï¼šHTTP ç›´è¿ XML + æ‰‹åŠ¨è§£æ"""
    try:
        params = dict(db="pubmed", id=",".join(id_list), rettype="abstract", retmode="xml")
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?" + urlencode(params)
        data = urlopen(Request(url, headers=UA_HDR), timeout=40).read()
        return list(_parse_pubmed_xml(data))
    except Exception as e:
        print(f"  â†ªï¸ HTTP XML å¤±è´¥ï¼š{e}")
        return []

def _efetch_http_text(id_list: List[str]) -> List[dict]:
    """å¤‡ç”¨2ï¼šHTTP ç›´è¿ TEXTï¼ˆMEDLINE æ–‡æœ¬ï¼‰ï¼Œç”¨ç²—ç•¥è§„åˆ™æŠ½é¢˜ç›®ä¸æ‘˜è¦"""
    try:
        params = dict(db="pubmed", id=",".join(id_list), rettype="abstract", retmode="text")
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?" + urlencode(params)
        raw = urlopen(Request(url, headers=UA_HDR), timeout=40).read().decode("utf-8", errors="ignore")
        # ç®€å•æŒ‰ PMID åˆ†å—
        blocks = [b.strip() for b in raw.split("\n\n") if b.strip()]
        out = []
        for blk in blocks:
            lines = [ln.strip() for ln in blk.splitlines() if ln.strip()]
            pmid, title, abstract = None, "", ""
            for ln in lines:
                if ln.startswith("PMID-"):
                    pmid = ln.split("PMID-")[-1].strip()
                elif ln.startswith("TI  -") or ln.startswith("TI -"):
                    title = ln.split("TI", 1)[-1].split("-", 1)[-1].strip()
                elif ln.startswith("AB  -") or ln.startswith("AB -"):
                    abstract += " " + ln.split("AB", 1)[-1].split("-", 1)[-1].strip()
            txt = " ".join([title, abstract]).strip()
            if pmid and txt:
                out.append({"id": f"pmid_{pmid}", "text": txt})
        return out
    except Exception as e:
        print(f"  â†ªï¸ HTTP TEXT å¤±è´¥ï¼š{e}")
        return []

def fetch_abstracts(pmids: List[str]) -> Iterable[dict]:
    BATCH = 20  # æ›´å°æ‰¹æ¬¡ï¼Œæé«˜æˆåŠŸç‡
    for i in range(0, len(pmids), BATCH):
        chunk = pmids[i:i+BATCH]
        print(f"â€¦ æŠ“å–æ‰¹æ¬¡ {i//BATCH+1} / {((len(pmids)-1)//BATCH)+1} ï¼ˆ{len(chunk)} ç¯‡ï¼‰")
        # 1) Entrez XML
        out = _efetch_entrez_xml(chunk)
        if not out:
            # 2) HTTP XML
            out = _efetch_http_xml(chunk)
        if not out:
            # 3) HTTP TEXT
            out = _efetch_http_text(chunk)
        if not out:
            print(f"â›” ä¸‰ç§æ–¹å¼éƒ½å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸€æ‰¹ï¼ˆç¤ºä¾‹ IDs: {chunk[:3]}â€¦ï¼‰")
        else:
            for item in out:
                yield item
        time.sleep(0.4)  # è½»å¾®é™é€Ÿï¼Œéµå®ˆç¤¼ä»ª

# ============== ä¸»æµç¨‹ =================
def main():
    retmax = int(sys.argv[1]) if len(sys.argv) >= 2 else 300

    here = os.path.dirname(os.path.abspath(__file__))
    root = os.path.abspath(os.path.join(here, os.pardir))
    os.chdir(root)

    os.makedirs("data", exist_ok=True)
    out_path = os.path.join("data", "docs.jsonl")

    print(f"ğŸ” æœç´¢ PubMedï¼š{QUERY}")
    pmids = search_pmids(QUERY, retmax=retmax)
    print(f"âœ… å‘½ä¸­ PMIDsï¼š{len(pmids)}")
    if not pmids:
        print("âŒ ä¸€ä¸ª PMID éƒ½æ²¡æ‹¿åˆ°ï¼Œé€€å‡ºã€‚")
        sys.exit(1)

    print("â¬ å¼€å§‹æŠ“å–æ‘˜è¦ï¼ˆå¤šé€šé“å…œåº•ï¼‰â€¦")
    n = 0
    with open(out_path, "w", encoding="utf-8") as f:
        for item in fetch_abstracts(pmids):
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
            n += 1

    print(f"âœ… å®Œæˆï¼šå†™å…¥ {n} æ¡åˆ° {out_path}")
    if n == 0:
        print("âš ï¸ æ²¡æŠ“åˆ°æ‘˜è¦ï¼Œå¯èƒ½ç½‘ç»œè¢«å¢™/é™æµã€‚å¯ç¨åé‡è¯•æˆ–æ¢å…³é”®è¯ã€‚")
    else:
        print("   ç°åœ¨æ‰§è¡Œï¼špython src\\build_index.py  é‡å»ºç´¢å¼•")

if __name__ == "__main__":
    main()
