# src/build_index.py
# åŠŸèƒ½ï¼šè¯»å– data/docs.jsonl é‡Œçš„æ®µè½ -> åˆ†å¥ -> ç”¨ en_core_sci_md æŠ½å®ä½“
#       ç”Ÿæˆç¨€ç–çŸ©é˜µ M(å¥å­xå®ä½“)ã€C(æ®µè½xå®ä½“) å¹¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•

import json, re, sys, os
import numpy as np
from scipy.sparse import csr_matrix
from tqdm import tqdm

def split_sentences(text: str):
    """è¶…ç®€å•åˆ†å¥ï¼šæŒ‰ . ? ! åçš„ç©ºæ ¼åˆ‡ã€‚ä½ ä¹Ÿå¯ä»¥æ›¿æ¢æˆæ›´å¼ºçš„åˆ†å¥å™¨ã€‚"""
    if not text:
        return []
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    return [s.strip() for s in sents if s.strip()]

def make_csr(pairs, n_rows, n_cols):
    """æŠŠ (row, col) å¯¹è½¬æˆç¨€ç–çŸ©é˜µ"""
    if pairs:
        rows, cols = zip(*pairs)
    else:
        rows, cols = [], []
    data = np.ones(len(rows), dtype=np.float32)
    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))

def load_md():
    """ä¼˜å…ˆä½¿ç”¨ä½ å·²å®‰è£…çš„ en_core_sci_mdï¼›åŠ è½½å¤±è´¥å°±æŠ¥é”™ï¼ˆæŒ‰ä½ è¦æ±‚ä¸è‡ªåŠ¨é™çº§ï¼‰"""
    try:
        import en_core_sci_md
        nlp = en_core_sci_md.load()
        print("âœ… å·²åŠ è½½ en_core_sci_md 0.5.4")
        return nlp
    except Exception as e:
        print("âŒ æ²¡æ‰¾åˆ° en_core_sci_mdï¼Œè¯·å…ˆå®‰è£…ï¼š")
        print("   python -m pip install <æœ¬åœ°è·¯å¾„æˆ–å®˜æ–¹ tar.gz>")
        raise

def main():
    # 0) åˆ‡åˆ°å·¥ç¨‹æ ¹ç›®å½•ï¼ˆä¿è¯è¾“å‡ºæ–‡ä»¶è½åœ¨æ ¹ç›®å½•ï¼‰
    here = os.path.dirname(os.path.abspath(__file__))
    root = os.path.abspath(os.path.join(here, os.pardir))
    os.chdir(root)

    # 1) åŠ è½½ NER
    nlp = load_md()

    # 2) è¯»å–æ®µè½
    docs_path = os.path.join("data", "docs.jsonl")
    try:
        docs = [json.loads(l) for l in open(docs_path, "r", encoding="utf-8") if l.strip()]
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ° data/docs.jsonlã€‚è¯·åˆ›å»ºåå†è¿è¡Œã€‚")
        sys.exit(1)
    if not docs:
        print("âŒ data/docs.jsonl ä¸ºç©ºã€‚")
        sys.exit(1)

    ent2id = {}           # å®ä½“å­—ç¬¦ä¸² -> ID
    sents = []            # æ‰€æœ‰å¥å­æ–‡æœ¬
    sent_docid = []       # æ¯ä¸ªå¥å­å¯¹åº”çš„æ®µè½ID
    para_ent_pairs = []   # (æ®µè½ç´¢å¼•, å®ä½“ID)
    sent_ent_pairs = []   # (å¥å­ç´¢å¼•, å®ä½“ID)
    doc_ids = []          # æ®µè½IDï¼ˆä¸ docs é¡ºåºä¸€è‡´ï¼‰
    doc_texts = {}        # æ®µè½ID -> åŸæ–‡

    print(f"ğŸ”§ å…± {len(docs)} ä¸ªæ®µè½ï¼Œå¼€å§‹åˆ†å¥ + æŠ½å®ä½“â€¦ï¼ˆæ¨¡å‹ï¼šen_core_sci_mdï¼‰")
    for di, d in enumerate(tqdm(docs)):
        pid = d["id"]; text = d["text"]
        doc_ids.append(pid)
        doc_texts[pid] = text

        ents_para = set()
        cur_sents = split_sentences(text)
        for sent in cur_sents:
            sid = len(sents)
            sents.append(sent)
            sent_docid.append(pid)

            # NERï¼šå®ä½“åªè¦å­—ç¬¦ä¸²ï¼Œä¸åšå¤æ‚è§„èŒƒåŒ–
            doc = nlp(sent)
            ents_sent = set(e.text.strip() for e in doc.ents if e.text.strip())
            for e in ents_sent:
                if e not in ent2id:
                    ent2id[e] = len(ent2id)
                sent_ent_pairs.append((sid, ent2id[e]))
            ents_para |= ents_sent

        for e in ents_para:
            para_ent_pairs.append((di, ent2id[e]))

    # 3) ç¨€ç–çŸ©é˜µ
    M = make_csr(sent_ent_pairs, n_rows=len(sents), n_cols=len(ent2id))
    C = make_csr(para_ent_pairs, n_rows=len(docs),  n_cols=len(ent2id))

    # 4) ä¿å­˜åˆ°å·¥ç¨‹æ ¹ç›®å½•
    np.savez_compressed(
        "index_tri_graph.npz",
        M_data=M.data, M_indices=M.indices, M_indptr=M.indptr, M_shape=M.shape,
        C_data=C.data, C_indices=C.indices, C_indptr=C.indptr, C_shape=C.shape
    )
    meta = {
        "docs": doc_ids,
        "doc_texts": doc_texts,
        "sents": sents,
        "sent_docid": sent_docid,
        "ent2id": ent2id
    }
    with open("index_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False)

    print("âœ… ç´¢å¼•å®Œæˆï¼š")
    print(f"   å¥å­æ•° = {len(sents)}")
    print(f"   å®ä½“æ•° = {len(ent2id)}")
    print(f"   æ®µè½æ•° = {len(docs)}")
    print("   å·²ç”Ÿæˆ index_tri_graph.npz ä¸ index_meta.json")

if __name__ == "__main__":
    main()
