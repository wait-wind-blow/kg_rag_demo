import argparse
import json
import os
from typing import List, Dict, Set, Tuple

import numpy as np
from sentence_transformers import SentenceTransformer


DOCS_PATH = os.path.join("data", "docs.jsonl")
VEC_EMB_PATH = os.path.join("data", "index_vec_emb.npy")


# ä½ å…³å¿ƒçš„â€œæ ‡å‡†è¯ç‰©åâ€åˆ—è¡¨ï¼ˆå’Œ qa_med_questions.jsonl é‡Œçš„ gold ä¸€è‡´ï¼‰
CANON_DRUGS: List[str] = [
    # Staph / MRSA ç›¸å…³
    "cefazolin",
    "ceftaroline",
    "cephalexin",
    "clindamycin",
    "daptomycin",
    "dicloxacillin",
    "doxycycline",
    "flucloxacillin",
    "gentamicin",
    "linezolid",
    "nafcillin",
    "oxacillin",
    "rifampin",
    "teicoplanin",
    "tetracycline",
    "trimethoprim-sulfamethoxazole",
    "vancomycin",

    # å°¿è·¯æ„ŸæŸ“
    "fosfomycin",
    "nitrofurantoin",

    # å‘¼å¸é“ / è‚ºç‚Ž
    "amoxicillin",
    "azithromycin",
    "clarithromycin",

    # å’½ç‚Ž
    "penicillin v",
    "benzathine penicillin g",

    # èœ‚çªç»‡ç‚Ž
    "amoxicillin-clavulanate",

    # å¹½é—¨èžºæ†èŒ
    "levofloxacin",
    "metronidazole",

    # æŠ—å‡å•èƒžèŒ
    "cefepime",
    "ceftazidime",
    "ciprofloxacin",
    "imipenem-cilastatin",
    "meropenem",
    "piperacillin-tazobactam",
]


# ä¸€ç‚¹ç®€å•çš„åˆ«åï¼ˆä¸»è¦æ˜¯è¿žå­—ç¬¦ / æ–œæ  / å¤§å†™é—®é¢˜ï¼‰
DRUG_SYNONYMS: Dict[str, List[str]] = {
    "trimethoprim-sulfamethoxazole": [
        "trimethoprim-sulfamethoxazole",
        "trimethoprim / sulfamethoxazole",
        "trimethoprim-sulphamethoxazole",
        "co-trimoxazole",
        "cotrimoxazole",
        "tmp-smx",
        "tmp / smx",
    ],
    "amoxicillin-clavulanate": [
        "amoxicillin-clavulanate",
        "amoxicillin / clavulanate",
        "amox-clav",
        "co-amoxiclav",
    ],
    "penicillin v": [
        "penicillin v",
        "penicillin vk",
    ],
    "benzathine penicillin g": [
        "benzathine penicillin g",
        "benzathine benzylpenicillin",
    ],
    # å…¶ä»–æ²¡å†™åˆ«åçš„ï¼Œå°±ç”¨åå­—æœ¬èº«åšåŒ¹é…
}


def build_drug_pattern_map() -> Dict[str, List[str]]:
    """æŠŠæ‰€æœ‰è¯åå’Œåˆ«åéƒ½å˜æˆå°å†™ï¼Œç”¨æ¥åšåŒ…å«åŒ¹é…ã€‚"""
    pat = {}
    for d in CANON_DRUGS:
        base = d.lower()
        pats = [base]
        extra = DRUG_SYNONYMS.get(d, [])
        pats.extend([e.lower() for e in extra])
        pat[d] = pats
    return pat


DRUG_PATTERNS = build_drug_pattern_map()


def load_docs(path: str = DOCS_PATH) -> List[Dict]:
    docs = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            docs.append(json.loads(line))
    return docs


def doc_text(doc: Dict) -> str:
    """ä»Žä¸€æ¡ doc é‡Œæ‹¼ä¸€ä¸ª 'æ ‡é¢˜ + æ‘˜è¦/æ­£æ–‡' çš„é•¿æ–‡æœ¬ã€‚"""
    parts = []
    title = doc.get("title") or doc.get("Title")
    abstract = doc.get("abstract") or doc.get("Abstract")
    text = doc.get("text") or doc.get("Text")

    if title:
        parts.append(title)
    if abstract:
        parts.append(abstract)
    elif text:
        parts.append(text)

    return "\n\n".join(parts)


def load_vec_index() -> np.ndarray:
    emb = np.load(VEC_EMB_PATH)
    return emb


def normalize_matrix(mat: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    norms = np.clip(norms, 1e-9, None)
    return mat / norms


def vec_search(
    query: str,
    model: SentenceTransformer,
    emb: np.ndarray,
    top_k: int = 10,
) -> Tuple[np.ndarray, np.ndarray]:
    """ç”¨å‘é‡ç›¸ä¼¼åº¦åš Top-K æ£€ç´¢ã€‚"""
    q_vec = model.encode([query], normalize_embeddings=True)[0]
    emb_norm = normalize_matrix(emb)
    scores = emb_norm @ q_vec
    idx = np.argsort(-scores)[:top_k]
    return idx, scores[idx]


def extract_drugs_from_text(text: str) -> List[str]:
    """åœ¨ä¸€æ®µå¤§æ–‡æœ¬é‡Œï¼Œçœ‹çœ‹æœ‰å“ªäº›è¯å / åˆ«åå‡ºçŽ°è¿‡ã€‚"""
    txt = text.lower()
    found: Set[str] = set()

    for canon_name, patterns in DRUG_PATTERNS.items():
        for p in patterns:
            if p in txt:
                found.add(canon_name)
                break

    return sorted(found)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("query", type=str, help="è¦æ£€ç´¢çš„é—®é¢˜")
    parser.add_argument(
        "--k",
        type=int,
        default=15,
        help="å‘é‡æ£€ç´¢ Top-K æ–‡çŒ®æ•°ï¼ˆé»˜è®¤ 15ï¼‰",
    )
    args = parser.parse_args()

    # 1. è¯»æ–‡çŒ® + å‘é‡
    docs = load_docs(DOCS_PATH)
    emb = load_vec_index()

    if emb.shape[0] != len(docs):
        raise RuntimeError(
            f"å‘é‡æ¡æ•° {emb.shape[0]} å’Œ docs æ¡æ•° {len(docs)} ä¸ä¸€è‡´ï¼Œ"
            f"è¯·ç¡®è®¤ build_vec_index_vec.py ç”¨çš„ä¹Ÿæ˜¯ {DOCS_PATH}ã€‚"
        )

    # 2. åŠ è½½å‘é‡æ¨¡åž‹
    model_name = "sentence-transformers/all-mpnet-base-v2"

    print(f"ðŸ§  åŠ è½½å‘é‡æ¨¡åž‹ï¼š{model_name}")
    model = SentenceTransformer(model_name)

    # 3. åšå‘é‡æ£€ç´¢
    idx, scores = vec_search(args.query, model, emb, top_k=args.k)

    # 4. æŠŠ Top-K æ–‡çŒ®æ‹¼æˆä¸€ä¸ªâ€œå¤§ä½œæ–‡â€ï¼Œä»Žé‡Œé¢æŠ½è¯å
    big_chunks = []
    for i in idx:
        doc = docs[int(i)]
        big_chunks.append(doc_text(doc))
    big_text = "\n\n".join(big_chunks)

    drugs = extract_drugs_from_text(big_text)

    # 5. æ‰“å°ç»“æžœ
    print("\n================ QUERY =================")
    print(args.query)

    print("=============== VEC+DRUGS ANSWER =============")
    if drugs:
        print(", ".join(drugs))
    else:
        print("(æ²¡æœ‰åœ¨ Top-K æ–‡çŒ®ä¸­æ‰¾åˆ°å¸¸è§è¯å)")

    print("============= CITATIONS (Top-K) =============")
    for rank, i in enumerate(idx):
        doc = docs[int(i)]
        pmid = doc.get("pmid") or doc.get("PMID") or "?"
        title = (doc.get("title") or doc.get("Title") or "").strip()
        if len(title) > 180:
            title = title[:177] + "..."
        print(f"[{rank+1}] pid=pmid_{pmid}  score={scores[rank]:.4f}")
        print(f"    {title}\n")


if __name__ == "__main__":
    main()
