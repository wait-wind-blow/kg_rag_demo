# -*- coding: utf-8 -*-
"""
å¯¹ data/qa_med_questions.jsonl é‡Œçš„é—®é¢˜ï¼Œ
ç”¨ã€ŒBM25 + æŠ½å–è¯ç‰©åˆ—è¡¨ã€çš„æ–¹å¼åšè¯„æµ‹ã€‚

æœ€ç»ˆè¾“å‡ºï¼š
- æ§åˆ¶å°æ‰“å°æ¯ä¸ªé—®é¢˜çš„ P/R/F1
- å†™ä¸€ä»½ CSV åˆ° runs/qa_med_eval_bm25.csv
"""

import os
import json
import re
import math
import csv
from collections import Counter

# ==== è·¯å¾„è®¾ç½® ====
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
RUNS_DIR = os.path.join(BASE_DIR, "runs")
os.makedirs(RUNS_DIR, exist_ok=True)

DOC_PATH = os.path.join(DATA_DIR, "docs.jsonl")
QA_PATH = os.path.join(DATA_DIR, "qa_med_questions.jsonl")
OUT_CSV = os.path.join(RUNS_DIR, "qa_med_eval_bm25.csv")

# ==== åŸºç¡€å·¥å…·ï¼šè¯»æ–‡ä»¶ ====

def load_docs(path):
    docs = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            docs.append(json.loads(line))
    return docs


def load_questions(path):
    qs = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            qs.append(obj)
    return qs


def get_text(doc):
    """æŠŠä¸€ç¯‡æ–‡çŒ®å˜æˆä¸€æ®µå¯æ£€ç´¢çš„æ–‡æœ¬ã€‚"""
    if doc.get("text"):
        return doc["text"]
    title = doc.get("title", "")
    abstract = doc.get("abstract", "")
    return (title + " " + abstract).strip()


# ==== BM25 å®ç°ï¼ˆç®€æ˜“ç‰ˆï¼Œè‡ªå·±ç®—ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰ ====

TOKEN_RE = re.compile(r"[A-Za-z]+")


def tokenize(text):
    """åªè¦è‹±æ–‡å­—æ¯ï¼Œç»Ÿç»Ÿå°å†™ã€‚"""
    return [m.group(0).lower() for m in TOKEN_RE.finditer(text)]


def build_bm25_index(docs):
    """
    ä¸ºæ‰€æœ‰æ–‡çŒ®é¢„è®¡ç®—ï¼š
    - æ¯ç¯‡çš„ term é¢‘ç‡
    - æ¯ç¯‡é•¿åº¦
    - æ¯ä¸ª term å‡ºç°åœ¨å“ªäº›æ–‡çŒ®é‡Œï¼ˆæ–‡æ¡£é¢‘ç‡ dfï¼‰
    - å¹³å‡æ–‡æ¡£é•¿åº¦ avgdl
    """
    doc_tfs = []
    doc_lens = []
    df = Counter()

    for doc in docs:
        tokens = tokenize(get_text(doc))
        tf = Counter(tokens)
        doc_tfs.append(tf)
        doc_lens.append(len(tokens))
        for term in tf.keys():
            df[term] += 1

    N = len(docs)
    avgdl = sum(doc_lens) / N if N > 0 else 0.0

    return {
        "doc_tfs": doc_tfs,
        "doc_lens": doc_lens,
        "df": df,
        "N": N,
        "avgdl": avgdl,
    }


def bm25_scores(query, index, k1=1.5, b=0.75):
    """å¯¹ä¸€ä¸ªæŸ¥è¯¢ï¼Œç®—å‡ºæ¯ç¯‡æ–‡çŒ®çš„ BM25 åˆ†æ•°ã€‚"""
    q_tokens = tokenize(query)
    doc_tfs = index["doc_tfs"]
    doc_lens = index["doc_lens"]
    df = index["df"]
    N = index["N"]
    avgdl = index["avgdl"]

    scores = []
    for i, tf in enumerate(doc_tfs):
        dl = doc_lens[i]
        score = 0.0
        for t in q_tokens:
            f = tf.get(t, 0)
            if f == 0:
                continue
            n_q = df.get(t, 0)
            if n_q == 0:
                continue
            # ç»å…¸ BM25 idf å…¬å¼
            idf = math.log((N - n_q + 0.5) / (n_q + 0.5) + 1.0)
            denom = f + k1 * (1 - b + b * dl / (avgdl + 1e-9))
            score += idf * f * (k1 + 1) / denom
        scores.append(score)
    return scores


# ==== è¯ç‰©æŠ½å–ï¼šè¯è¡¨ + æ­£åˆ™ ====

def build_drug_lexicon(questions):
    """
    å·æ‡’ä½†å®ç”¨çš„åšæ³•ï¼š
    â€”â€”ç›´æ¥æŠŠ qa æ–‡ä»¶é‡Œæ‰€æœ‰ gold_drugs åˆå¹¶æˆä¸€ä¸ªè¯è¡¨ã€‚
    è¿™æ ·èƒ½ä¿è¯ï¼šå‡¡æ˜¯ gold é‡Œæœ‰çš„è¯åï¼Œåªè¦å‡ºç°åœ¨æ–‡çŒ®æ–‡æœ¬é‡Œï¼Œéƒ½æœ‰æœºä¼šè¢«åŒ¹é…å‡ºæ¥ã€‚
    """
    lex = set()
    for q in questions:
        for d in q.get("gold_drugs", []):
            if d:
                lex.add(d.strip())
    # å…¨éƒ¨è½¬å°å†™ï¼Œæ–¹ä¾¿åŒ¹é…
    return {d.lower() for d in lex}


def build_drug_regex(lexicon):
    """
    æ ¹æ®è¯ç‰©è¯è¡¨æ„é€ ä¸€ä¸ªå¤§æ­£åˆ™ï¼š
    \b(drug1|drug2|...)\b
    """
    if not lexicon:
        return None
    # é•¿çš„è¯åæ”¾å‰é¢ï¼Œé¿å…çŸ­è¯ä¹±åŒ¹é…
    parts = [re.escape(d) for d in sorted(lexicon, key=len, reverse=True)]
    pattern = r"\b(" + "|".join(parts) + r")\b"
    return re.compile(pattern, re.I)


def extract_drugs(text, drug_re):
    if not drug_re:
        return []
    found = set(m.group(0).lower() for m in drug_re.finditer(text))
    return sorted(found)


# ==== å•é¢˜è¯„æµ‹ ====

def eval_one(q, docs, index, drug_re, top_k=20):
    qid = q.get("id", "?")
    qtext = q["question"]
    gold = {d.lower() for d in q.get("gold_drugs", [])}

    scores = bm25_scores(qtext, index)
    # å–åˆ†æ•°æœ€é«˜çš„ top_k ç¯‡æ–‡çŒ®
    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    combined_text = "\n\n".join(get_text(docs[i]) for i in top_idx)

    pred_list = extract_drugs(combined_text, drug_re)
    pred = set(pred_list)

    tp = len(pred & gold)
    fp = len(pred - gold)
    fn = len(gold - pred)

    prec = tp / (tp + fp) if tp + fp > 0 else 0.0
    rec = tp / (tp + fn) if tp + fn > 0 else 0.0
    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0.0

    print(f"\n=== è¯„æµ‹é—®é¢˜ {qid} ===")
    print(qtext)
    print(f"TP={tp} FP={fp} FN={fn}  P={prec:.4f} R={rec:.4f} F1={f1:.4f}")
    print("é¢„æµ‹è¯ç‰©ï¼š", ";".join(sorted(pred)) if pred else "(æ— )")

    return {
        "id": qid,
        "question": qtext,
        "gold_size": len(gold),
        "pred_size": len(pred),
        "tp": tp,
        "fp": fp,
        "fn": fn,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "gold_drugs": ";".join(sorted(gold)),
        "pred_drugs": ";".join(sorted(pred)),
    }


# ==== ä¸»å‡½æ•° ====

def main():
    print(f"ğŸ“„ è¯»å–é—®é¢˜æ–‡ä»¶ï¼š{QA_PATH}")
    questions = load_questions(QA_PATH)
    print(f"âœ… å…±è¯»å– {len(questions)} ä¸ªé—®é¢˜")

    print(f"ğŸ“„ è¯»å–è¯­æ–™ï¼š{DOC_PATH}")
    docs = load_docs(DOC_PATH)
    print(f"âœ… æ–‡çŒ®æ¡æ•°ï¼š{len(docs)}")

    print("ğŸ§® æ„å»º BM25 ç´¢å¼•â€¦")
    index = build_bm25_index(docs)

    print("ğŸ“š æ ¹æ® gold_drugs æ„å»ºè¯ç‰©è¯è¡¨â€¦")
    drug_lex = build_drug_lexicon(questions)
    drug_re = build_drug_regex(drug_lex)
    print(f"âœ… è¯è¡¨å¤§å°ï¼š{len(drug_lex)}")

    results = []
    micro_tp = micro_fp = micro_fn = 0

    for q in questions:
        r = eval_one(q, docs, index, drug_re, top_k=20)
        results.append(r)
        micro_tp += r["tp"]
        micro_fp += r["fp"]
        micro_fn += r["fn"]

    # è®¡ç®— micro / macro
    micro_p = micro_tp / (micro_tp + micro_fp) if micro_tp + micro_fp > 0 else 0.0
    micro_r = micro_tp / (micro_tp + micro_fn) if micro_tp + micro_fn > 0 else 0.0
    micro_f1 = 2 * micro_p * micro_r / (micro_p + micro_r) if micro_p + micro_r > 0 else 0.0

    macro_p = sum(r["precision"] for r in results) / len(results)
    macro_r = sum(r["recall"] for r in results) / len(results)
    macro_f1 = sum(r["f1"] for r in results) / len(results)

    # å†™ CSV
    with open(OUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "id", "question",
            "gold_size", "pred_size",
            "tp", "fp", "fn",
            "precision", "recall", "f1",
            "gold_drugs", "pred_drugs"
        ])
        for r in results:
            writer.writerow([
                r["id"], r["question"],
                r["gold_size"], r["pred_size"],
                r["tp"], r["fp"], r["fn"],
                f"{r['precision']:.4f}",
                f"{r['recall']:.4f}",
                f"{r['f1']:.4f}",
                r["gold_drugs"],
                r["pred_drugs"],
            ])

    print("=" * 80)
    print(f"âœ… å·²å†™å‡ºè¯„æµ‹ç»“æœåˆ°ï¼š{OUT_CSV}")
    print(f"ğŸ”¢ å¾®å¹³å‡ (micro)ï¼šP={micro_p:.3f}  R={micro_r:.3f}  F1={micro_f1:.3f}")
    print(f"ğŸ”¢ å®å¹³å‡ (macro)ï¼šP={macro_p:.3f}  R={macro_r:.3f}  F1={macro_f1:.3f}")


if __name__ == "__main__":
    main()
